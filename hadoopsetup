#!/bin/bash
# This is a nondestructive script which can be run many times without banging on the files adding the 
# same info again and again. I is designed for 64 bit ubuntu

#Variables START HERE
#Hadoop User
HADOOPUSER="hduser"
HADOOPGRP="hadoop"
# Password only used if not passed in as arg on commandline.
HADOOPPASS="password"


# Hadoop
HADOOPVERSION=hadoop-2.4.0

#Java 
JAVA_REQ_VER="1.7.0_55"

#NFS
NFSLOCALMNT="/mnt/share"
NFSSERVERMNT="192.168.0.10:/share"
FSTABSTR="${NFSSERVERMNT}      ${NFSLOCALMNT}    nfs async,nfsvers=3,actimeo=0,tcp,rsize=32768,wsize=32768    0       0"
#Variables STOP HERE


# must be root
if [[ $EUID -ne 0 ]]; then
   echo "This script must be run as root" 1>&2
   exit 1
fi

# install some required packages
apt-get -qq update > /dev/null
apt-get install -y -qq screen openssh-server nfs-common rsync 

# purge open java
apt-get -y purge openjdk-\* icedtea-\* icedtea6-\* > /dev/null


##setup directories
# create nfs mount for zotac
if [ ! -d ${NFSLOCALMNT} ]; then
    mkdir -p ${NFSLOCALMNT}
fi
if [ ! -d "/usr/local/java" ]; then
    mkdir -p "/usr/local/java"
fi

# mount nfs

grep "${FSTABSTR}" /etc/fstab >/dev/null  || echo ${FSTABSTR} >> /etc/fstab 
mount -a

#setup hadoop user, use 1st arg supplied on cli, but if empty, use the variable set at the top of script.
if [ -z "$1" ] ; then
ARGPASS=${HADOOPPASS}
else
ARGPASS=$1
fi 

addgroup ${HADOOPGRP}
PASS=`perl -e "print crypt($ARGPASS, $ARGPASS);"`
useradd -d /home/${HADOOPUSER} -m ${HADOOPUSER} -p "$PASS" -G ${HADOOPGRP},sudo -s /bin/bash

# setup ssh (yes this is bad)
cd /mnt/share/hadoop/other
tar vxzf ssh.gz -C /home/hduser

# copy hadoop 
if [ ! -d "/usr/local/hadoop" ]; then
tar xzf /mnt/share/hadoop/hadoop/${HADOOPVER}.tar.gz -C /usr/local
cd /usr/local
mv ${HADOOPVER} hadoop
chown -R ${HADOOPUSER}:${HADOOPGRP} hadoop
echo "Copied hadoop"
else
echo "Hadoop folder already exists - delete /usr/local/hadoop to re-install"
fi

# copy java  if not setup already.
JAVA_VER=$(java -version 2>&1 | sed 's#java version "\(.*\)\(.*\)\.*"#\1\2#; 1q')
echo "Local java is ${JAVA_VER}"
echo "Required java is ${JAVA_REQ_VER}"
if [ ${JAVA_VER} != ${JAVA_REQ_VER} ]; then
  echo "Updateing java to ${JAVA_REQ_VER} ...."
  sleep 5s
  cp -r  /mnt/share/hadoop/java/64/jdk1.7.0_55/ /usr/local/java/
  cd /usr/local/java
  # configure java
  update-alternatives --install "/usr/bin/java" "java" "/usr/local/java/jdk1.7.0_55/bin/java" 1
  update-alternatives --install "/usr/bin/javac" "javac" "/usr/local/java/jdk1.7.0_55/bin/javac" 1
  update-alternatives --install "/usr/bin/javaws" "javaws" "/usr/local/java/jdk1.7.0_55/bin/javaws" 1
  update-alternatives --install "/usr/bin/jps" "jps" "/usr/local/java/jdk1.7.0_55/bin/jps" 1
  update-alternatives --set java /usr/local/java/jdk1.7.0_55/bin/java
  update-alternatives --set javac /usr/local/java/jdk1.7.0_55/bin/javac
  update-alternatives --set javaws /usr/local/java/jdk1.7.0_55/bin/javaws
  update-alternatives --set jps /usr/local/java/jdk1.7.0_55/bin/jps
  chmod a+x /usr/bin/java
  chmod a+x /usr/bin/javac
  chmod a+x /usr/bin/javaws
  chmod a+x /usr/bin/jps
else
echo "no changes to java required"
fi
sleep 5s

## setup environment variables
profile_file=/home/${HADOOPUSER}/.bashrc
if ! grep -q '#Hadoop variables' "${profile_file}" ; then
cat >> ${profile_file} << EOF
#Hadoop variables
export JAVA_HOME=/usr/local/java/jdk1.7.0_55/
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
#end of paste
EOF
echo "Updated .bashrc for hadoop variables"
source /home/${HADOOPUSER}/.bashrc
chown ${HADOOPUSER}:${HADOOPGRP} /home/${HADOOPUSER}/.bashrc
else
echo "Skipping .bashrc update"
fi

# configure hadoop
cd /usr/local/hadoop/etc/hadoop
if [ ! -f "updated" ]; then
touch mapred-site.xml
touch hdfs-site.xml
touch yarn-site.xml
mv hdfs-site.xml hdfs-site.xml.old
mv mapred-site.xml mapred-site.xml.old
mv yarn-site.xml yarn-site.xml.old
cp /mnt/share/hadoop/other/hdfs-site.xml .
cp /mnt/share/hadoop/other/mapred-site.xml .
cp /mnt/share/hadoop/other/yarn-site.xml .
touch updated
echo "Updated hdfs,yarn,mapred files"
else
echo "''updated'' file found - no changes made to hdfs,yarn,mapred files"
fi
yes | cp /mnt/share/hadoop/other/slaves .
chown -R ${HADOOPUSER}:${HADOOPGRP} .


## update a central hosts file (not using DNS) All hadoop masters and slaves grab a common host file.
# you may need to update the subnet mask to reflect your network structure.
SUBNET=255.255.255.0
HOSTFILE=hosts
SLAVEFILE=slaves
REMOTEHOSTDIR=/mnt/share/hadoop/other
LOCALHOSTDIR=/etc
HOSTNAME=`hostname`
DOMAIN="tux.localdomain"
IPADDRESS=`ifconfig | sed -n "/inet addr:.*${SUBNET}/{s/.*inet addr://; s/ .*//; p}"`
HOSTENTRY="$IPADDRESS   $HOSTNAME.$DOMAIN   $HOSTNAME" 


if ! grep -q ${HOSTNAME} ${REMOTEHOSTDIR}/${HOSTFILE} ; then
echo ${HOSTENTRY} >> ${REMOTEHOSTDIR}/${HOSTFILE}
echo "added new entry for ${HOSTNAME} into ${HOSTFILE}"
fi

if ! grep -q ${HOSTNAME} ${REMOTEHOSTDIR}/${SLAVEFILE} ; then
echo ${HOSTNAME} >> ${REMOTEHOSTDIR}/${SLAVEFILE}
echo "added new entry for ${HOSTNAME} into ${SLAVEFILE}"
fi


# pull down and overwrite current host file.
# then append "localhost" to the relevent entry
mv ${LOCALHOSTDIR}/${HOSTFILE} ${LOCALHOSTDIR}/${HOSTFILE}.old
cp ${REMOTEHOSTDIR}/${HOSTFILE} ${LOCALHOSTDIR}/${HOSTFILE}
sed -i "s#^${HOSTENTRY}#${HOSTENTRY}  localhost#" ${LOCALHOSTDIR}/${HOSTFILE}

## Update cron to pull central hosts file.
# use 1 file for hosts and pull it often using cron. We also need to add localhost to the right entry.
#CRON="1 2 3 4 5 /root/bin/backup.sh"
#cat < (crontab -l) |grep -v "${CRON}" < (echo "${CRON}")
